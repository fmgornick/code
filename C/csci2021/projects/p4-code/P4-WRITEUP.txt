                              ____________

                               P4 WRITEUP
                              ____________


- Name: (FILL THIS in)
- NetID: (THE kauf0095 IN kauf0095@umn.edu)

Answer the questions below according to the project specification. Write
your answers directly in this text file and submit it along with your
code.


PROBLEM 1: sumdiag_OPTM()
=========================

  Do your timing study on csel-keller1250-NN.cselabs.umn.edu


(A) Paste Source Code
~~~~~~~~~~~~~~~~~~~~~

  Paste a copy of your source code for the function `sumdiag_OPTM()'

  ####################### YOUR ANSWER HERE #########################

  int sumdiag_OPTM(matrix_t mat, vector_t vec) {
    int len=vec.len, rows=mat.rows, cols=mat.cols;
    for (int i=0; i<len; i++) VSET(vec,i,0);

    if (cols % 2 == 0) {
      for (int r=0; r<rows; r++) {
        for (int c=0; c<cols-1; c+=2) {
          vec.data[c - r + cols - 1] += MGET(mat,r,c);
          vec.data[c - r + cols] += MGET(mat,r,c+1);
        }
      }
    }

    else {
      for (int r=0; r<rows; r++) {
        for (int c=0; c<cols-2; c+=2) {
          vec.data[c - r + cols - 1] += MGET(mat,r,c);
          vec.data[c - r + cols] += MGET(mat,r,c+1);
        }
        vec.data[len - r - 1] += MGET(mat,r,cols-1);
      }
    }

    return 0;
  }

  ##################################################################


(B) Timing on csel-kh1250-NN
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  Paste a copy of the results of running `sumdiag_benchmark' on
  csel-kh1250-NN.cselabs.umn.edu in the space below which shows how your
  performance optimizations improved on the baseline codes.

  ####################### YOUR ANSWER HERE #########################

  ==== Matrix Diagonal Sum Benchmark Version 2 ====
  ------ Tuned for csel-kh1250-NN machines --------
    SIZE       BASE       OPTM  SPDUP POINTS 
    512  5.6740e-03 1.2190e-03   4.65   3.65 
    1024 6.1700e-03 2.4080e-03   2.56   1.56 
    1101 7.9020e-03 2.7860e-03   2.84   1.84 
    2048 8.8077e-02 1.0255e-02   8.59   7.59 
    4099 3.7282e-01 4.2851e-02   8.70   7.70 
    6001 9.3711e-01 9.2441e-02  10.14   9.14 
    8191 2.3029e+00 1.7271e-01  13.33  12.33 
  RAW POINTS: 43.81
  TOTAL POINTS: 35 / 35

  ##################################################################


(C) Optimizations
~~~~~~~~~~~~~~~~~

  Describe in some detail the optimizations you used to speed the code
  up.  THE CODE SHOULD CONTAIN SOME COMMENTS already to describe these
  but in the section below, describe in English the techniques you used
  to make the code run faster.  Format your descriptions into discrete
  chunks such as.
        Optimization 1: Blah bla blah... This should make run
        faster because yakkety yakeety yak.

        Optimization 2: Blah bla blah... This should make run
        faster because yakkety yakeety yak.

        ...  Optimization N: Blah bla blah... This should make run
        faster because yakkety yakeety yak.
  Full credit solutions will have a least two optimizations and describe
  WHY these improved performance in at least a couple sentences.

  ####################### YOUR ANSWER HERE #########################

  Optimization 1: I created variables len, rows, cols which come from the 
  vector_t and matrix_t struct respectively.  This should make the function run
  faster because we don't need to enter the struct every time we need to access
  a value.

  Optimization 2: I iterated through the matrix column by column, then row by 
  row.  This should make the function run faster because each index is being 
  accessed sequentially, meaning that there's no needless jumping.  This allows
  a big chunk of the matrix (or maybe even the whole matrix) to be cached, and
  make it very fast accessing values.

  Optimization 3: I just did all the calculations for what the index should be
  for the vector in the same line as where I accessed it's index.  I did this
  instead of declaring new variables, because these variables are only being
  used once per iteration of the column loop anyway.

  Optimization 4: I had the vector update two values in each for loop iteration
  of the columns, this allows for the two operations to be done in parallel, 
  essentially cutting the time to sum up all the diagonals in half.  There was 
  also the case that the number of columns was odd, so I added a conditional 
  and made that case do one extra index per column loop.

  ##################################################################


PROBLEM 2: Timing Search Algorithms
===================================

  Do your timing study on csel-kh1250-NN.cselabs.umn.edu. In most cases,
  report times larger than 1e-03 seconds as times shorter than this are
  unreliable. Run searches for more repetitions to lengthen run times.


(A) Min Size for Algorithmic Differences
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  Determine the size of input array where one starts to see a measurable
  difference in the performance of the linear and logarithmic
  algorithms.  Produce a timing table which includes all algorithms
  which clearly demonstrates an uptick in the times associated with some
  while others remain much lower.  Identify what size this appears to be
  a occur.

  ####################### YOUR ANSWER HERE #########################

  For me, I start to see a noticeable difference for each structure when the
  length is around 1024 or 2^10.  This is with the repetitions set at 1000 
  though.  With a length of 512 there is also a noticeable difference with each
  time exceeding 1.0000e-02, but it becomes more clear with a size of 1024.

  > ./search_benchmark 4 12 1000

  LENGTH   SEARCHES        array         list       binary         tree
      16      32000   1.4260e-03   1.4640e-03   1.1070e-03   1.0080e-03
      32      64000   1.0900e-03   1.3300e-03   6.1700e-04   5.1400e-04
      64     128000   4.5000e-03   6.8260e-03   1.2710e-03   8.4600e-04
     128     256000   1.2094e-02   2.3335e-02   2.6650e-03   1.9420e-03
     256     512000   4.0208e-02   8.8198e-02   7.3560e-03   7.5760e-03
     512    1024000   1.6289e-01   3.5424e-01   2.8403e-02   2.6844e-02
    1024    2048000   5.3737e-01   1.5929e+00   6.6744e-02   5.6646e-02
    2048    4096000   2.1252e+00   1.8336e+01   1.5820e-01   1.3462e-01
    4096    8192000   8.2376e+00   9.2566e+01   3.1703e-01   2.8832e-01

  ##################################################################


(B) Linear Search in List vs Array
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  Determine whether the linear array and linked list search remain
  approximately at the same performance level as size increases to large
  data or whether one begins to become favorable over other. Determine
  the approximate size at which this divergence becomes obvious. Discuss
  reasons WHY this difference arises.

  ####################### YOUR ANSWER HERE #########################

  With really low structure sizes the linked list actually starts off better.
  When the size ranges from 2 to 16 the linked usually performs just a bit 
  faster.  The array starts to perform better at a length of around 32.  Then 
  at a size of 64, the array starts to run over twice as fast as the linked 
  list.  But it isn't until we reach a size of 4096 that the array search
  becomes an order of magnitude better.  

  This is most likely because large sums of memory can be cached for easy 
  access by the cpu when using an array, but when working with a linked list, 
  you're using pointers to hop around in main memory, this can slow it down 
  quite a bit.  This also helps explain why the linked list actually runs a 
  little faster with smaller sizes, because caching can be an expensive 
  operation, and isn't really useful until you're using a big array.

  > ./search_benchmark 1 12 1000 al

  LENGTH   SEARCHES        array         list
       2       4000   3.9000e-05   3.7000e-05
       4       8000   9.1000e-05   6.0000e-05
       8      16000   1.4100e-04   1.1100e-04
      16      32000   3.1900e-04   2.7900e-04
      32      64000   8.6700e-04   1.0920e-03
      64     128000   3.8260e-03   8.0350e-03
     128     256000   1.4006e-02   2.8044e-02
     256     512000   4.9176e-02   1.0207e-01
     512    1024000   1.6178e-01   3.8646e-01
    1024    2048000   6.2311e-01   1.8057e+00
    2048    4096000   2.3245e+00   1.9070e+01
    4096    8192000   9.0837e+00   8.8630e+01

  ##################################################################


(C) Binary Search in Tree vs Array
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  Compare the binary array search and binary tree search on small to
  very large arrays. Determine if there is a size at which the
  performance of these two begins to diverge. If so, describe why this
  might be happening based on your understanding of the data structures
  and the memory system. If not, describe why you believe there is
  little performance difference between the two.

  ####################### YOUR ANSWER HERE #########################

  These two structures seem to work at the same speed no matter the size.  The
  tree structure is usually a bit more efficient than the array for the most
  part.  The reason the tree is slightly faster is because when running the 
  binary operations on the array, it'll most likely cache the elements arround
  a spoot, but binary search hops around and isn't very sequential, so the 
  extra expense of caching array elements ultimately doesn't help speed it up
  much.  This difference is only minimal, and because the searching method is
  O(n*log(n)) for both structures, they don't diverge.

  > ./search_benchmark 1 12 1000 bt

  LENGTH   SEARCHES       binary         tree
       2       4000   6.9000e-05   6.6000e-05
       4       8000   1.4500e-04   1.4200e-04
       8      16000   2.9000e-04   1.4400e-04
      16      32000   3.6900e-04   3.3500e-04
      32      64000   5.5200e-04   4.7000e-04
      64     128000   1.3330e-03   8.3600e-04
     128     256000   2.6700e-03   2.1300e-03
     256     512000   7.6750e-03   5.7210e-03
     512    1024000   3.0182e-02   2.7762e-02
    1024    2048000   7.1390e-02   6.0444e-02
    2048    4096000   1.5610e-01   1.4007e-01
    4096    8192000   3.2925e-01   3.2310e-01

  ##################################################################


(D) Caching Effects on Algorithms
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  It is commonly believed that memory systems that feature a Cache will
  lead to arrays performing faster than linked structures such as Linked
  Lists and Binary Search Trees. Describe whether your timings confirm
  or refute this belief.  Address both types of algorithms in your
  answer:
  - What effects does Cache have on Linear Search in arrays and lists
    and why?
  - What effects does Cache have on Binary Search in arrays and trees
    and why?

  ####################### YOUR ANSWER HERE #########################

  Caching has a very positive affect when using linear search do go through an
  array.  It definitely shows in the results as well.  As the sizes get larger,
  the arrays become more and more efficient than the linked lists.  This is
  because we're moving sequentially through both, so when caching a hunk of
  memory, most of that memory will actually be relevant.  That's why the linear
  search array is more efficient than the linked list at higher sizes.

  With binary search on the other hand, caching doesn't really help.  The 
  results actually show that the binary search tree is usually a bit more 
  efficient than the binary search array.  This is because when we're on a
  specific element in a binary array.  The next element won't be arround the 
  current element, instead we will need to jump to another location on the 
  array.  The computer has no clue which elements we're jumping to until we
  make the jump, so it'll just take a bunch of elements near it, and most 
  likely we won't actually need any of them.  This essentially makes caching
  useless for binary search.

  ##################################################################

